{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen2-7B-Instruct å¾®è°ƒ\n",
    "å¾®è°ƒçš„åŸºæœ¬æ–¹æ³•å‚ç…§ [Qwen2-7B-Instruct Lora å¾®è°ƒ](https://blog.csdn.net/xiaobing259/article/details/140594017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug  9 11:51:38 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:4F:00.0 Off |                    0 |\n",
      "| N/A   68C    P0             287W / 300W |  48472MiB / 81920MiB |     88%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:52:00.0 Off |                    0 |\n",
      "| N/A   72C    P0             294W / 300W |  46678MiB / 81920MiB |     93%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:56:00.0 Off |                    0 |\n",
      "| N/A   70C    P0             272W / 300W |  53307MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:57:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              68W / 300W |  71489MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100 80GB PCIe          Off | 00000000:CE:00.0 Off |                    0 |\n",
      "| N/A   84C    P0             212W / 300W |  35415MiB / 81920MiB |     78%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100 80GB PCIe          Off | 00000000:D1:00.0 Off |                    0 |\n",
      "| N/A   81C    P0             298W / 300W |  60089MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100 80GB PCIe          Off | 00000000:D5:00.0 Off |                    0 |\n",
      "| N/A   71C    P0             302W / 300W |  25405MiB / 81920MiB |     89%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100 80GB PCIe          Off | 00000000:D6:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              66W / 300W |  70099MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   1095299      C   python                                    46388MiB |\n",
      "|    0   N/A  N/A   2058100      C   python                                     2072MiB |\n",
      "|    1   N/A  N/A   1109998      C   python                                    46252MiB |\n",
      "|    1   N/A  N/A   3030986      C   python                                      414MiB |\n",
      "|    2   N/A  N/A   1318570      C   ...iniconda3/envs/datawhale/bin/python    53298MiB |\n",
      "|    3   N/A  N/A   2758239      C   python                                    71472MiB |\n",
      "|    4   N/A  N/A   1297818      C   python                                    21708MiB |\n",
      "|    4   N/A  N/A   1805895      C   python                                     1080MiB |\n",
      "|    4   N/A  N/A   1806474      C   .../miniconda3/envs/Wav2Lip/bin/python     1846MiB |\n",
      "|    4   N/A  N/A   1806475      C   .../miniconda3/envs/Wav2Lip/bin/python     7478MiB |\n",
      "|    4   N/A  N/A   1806476      C   .../miniconda3/envs/Wav2Lip/bin/python     1866MiB |\n",
      "|    4   N/A  N/A   1806477      C   .../miniconda3/envs/Wav2Lip/bin/python     1410MiB |\n",
      "|    5   N/A  N/A   1226879      C   ...iniconda3/envs/datawhale/bin/python    60080MiB |\n",
      "|    6   N/A  N/A   2834615      C   ...iniconda3/envs/datawhale/bin/python    25396MiB |\n",
      "|    7   N/A  N/A   3009755      C   python3                                   70090MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "cuda:4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pdb\n",
    "import json\n",
    "import torch\n",
    "import optuna\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig, EarlyStoppingCallback\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from data_processing.utils import extract_from_output\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "GPU_ID = 4\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{GPU_ID}\"  # åªä½¿ç”¨ä¸€å¼ æ˜¾å¡\n",
    "device = torch.device(f\"cuda:{GPU_ID}\" if torch.cuda.is_available() else \"cpu\")  # æŒ‡å®šä½¿ç”¨çš„è®¾å¤‡\n",
    "device_map = {\"\": f\"cuda:{GPU_ID}\"}\n",
    "print(device)\n",
    "\n",
    "# å¤š GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1'\n",
    "# device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device_map = {: ''}\n",
    "\n",
    "# ä»»åŠ¡ä»£å·\n",
    "TASK_ID = '5000'\n",
    "# torch.cuda.device_count()\n",
    "# # æŒ‡å®šä½¿ç”¨ç¬¬1å·GPU\n",
    "# torch.cuda.set_device(0)\n",
    "# # æ£€æŸ¥è®¾ç½®æ˜¯å¦ç”Ÿæ•ˆ\n",
    "# print(torch.cuda.current_device())  # åº”è¯¥è¾“å‡º1\n",
    "# print(torch.cuda.get_device_name(0))  # è¾“å‡ºç¬¬1å·GPUçš„åç§°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 35149 rows.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', '__index_level_0__'],\n",
       "    num_rows: 4500\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # è¯»å–åŸæ•°æ®é›†\n",
    "# df1 = pd.read_json('/data/disk4/home/chenrui/ai-project/logical_reasoning/data/input/raw/round1_train_data_instruction.json')\n",
    "# /data/disk4/home/chenrui/ai-project/logical_reasoning/data/input/train_data_raw_new_inst.json  # åŒ…å«åŸæ•°æ®é›†å™ªå£°\n",
    "# è¯»å–æ–°å¢æ•°æ®é›†\n",
    "# df2 = pd.read_json('/data/disk4/home/chenrui/ai-project/logical_reasoning/data/input/new/train_data_new_inst.json')  # äººå·¥å‡å¼±å™ªå£°åçš„æ•°æ®é›†\n",
    "\n",
    "# # # è¯»å–ç”Ÿæˆæ•°æ®é›†\n",
    "# df3 = pd.read_json('/data/disk4/home/chenrui/ai-project/logical_reasoning/data/input/gpt/gpt4_4590_806_inst.json')\n",
    "\n",
    "# # åˆå¹¶ä¸¤ä¸ª DataFrame\n",
    "# merged_df = pd.concat([df2, df3], ignore_index=True)\n",
    "# print(len(merged_df))\n",
    "\n",
    "# æ•°æ®é›†ï¼ˆæŒ‡ä»¤é›†ï¼‰è·¯å¾„\n",
    "data_path = '/data/disk4/home/chenrui/ai-project/logical_reasoning/LLaMA-Factory/data/logical_problems_large.json'\n",
    "\n",
    "# # åˆå¹¶å¥½çš„æŒ‡ä»¤é›†\n",
    "merged_df = pd.read_json(data_path)\n",
    "\n",
    "# æ£€æŸ¥DataFrameçš„é•¿åº¦\n",
    "num_rows = len(merged_df)\n",
    "print(f\"The DataFrame has {num_rows} rows.\")\n",
    "\n",
    "# å¦‚æœDataFrameçš„è¡Œæ•°å¤§äºæˆ–ç­‰äº5000ï¼ŒéšæœºæŠ½å–5000è¡Œ\n",
    "if num_rows >= 5000:\n",
    "    random_seed = 42\n",
    "    sample_df = merged_df.sample(n=5000, random_state=random_seed)\n",
    "else:\n",
    "    print(\"The DataFrame has fewer than 5000 rows. Returning the entire DataFrame.\")\n",
    "    sample_df = merged_df\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "train_data, val_data = train_test_split(sample_df, test_size=0.1, random_state=42)  # 80% è®­ç»ƒï¼Œ20% éªŒè¯\n",
    "# print(val_data)\n",
    "\n",
    "# å°†éªŒè¯é›†ä¿å­˜ä¸ºJSONæ ¼å¼åˆ°tmpæ–‡ä»¶å¤¹\n",
    "tmp_dir = '/data/disk4/home/chenrui/ai-project/logical_reasoning/data/input/tmp'\n",
    "os.makedirs(tmp_dir, exist_ok=True)  # åˆ›å»ºtmpæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨çš„è¯ï¼‰\n",
    "val_data.to_json(os.path.join(tmp_dir, 'validation_data.json'), orient='records', force_ascii=False)\n",
    "\n",
    "# å°†æ•°æ®é›†è½¬æ¢ä¸ºDatasetå¯¹è±¡\n",
    "train_ds = Dataset.from_pandas(train_data)\n",
    "val_ds = Dataset.from_pandas(val_data)\n",
    "\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹\n",
    "model_path = '/data/disk4/home/chenrui/.cache/modelscope/hub/qwen/Qwen2-Math-7B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device_map, torch_dtype=torch.bfloat16)\n",
    "model.enable_input_require_grads() # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹æ—¶ï¼Œè¦æ‰§è¡Œè¯¥æ–¹æ³•\n",
    "model.dtype # æŸ¥çœ‹ç²¾åº¦\n",
    "print(next(model.parameters()).device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4500/4500 [00:11<00:00, 407.47 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [00:01<00:00, 332.69 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 4500\n",
       " }),\n",
       " '<|im_start|>system\\nä½ æ˜¯ä¸€ä¸ªé€»è¾‘æ¨ç†ä¸“å®¶ï¼Œæ“…é•¿è§£å†³é€»è¾‘æ¨ç†é—®é¢˜ã€‚<|im_end|>\\n<|im_start|>user\\n\\nä½ æ˜¯ä¸€ä¸ªé€»è¾‘æ¨ç†ä¸“å®¶ï¼Œæ“…é•¿è§£å†³é€»è¾‘æ¨ç†é—®é¢˜ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªé€»è¾‘æ¨ç†çš„é¢˜ç›®ï¼Œå½¢å¼ä¸ºå•é¡¹é€‰æ‹©é¢˜ã€‚æ‰€æœ‰çš„é—®é¢˜éƒ½æ˜¯ï¼ˆclose-world assumptionï¼‰é—­ä¸–ç•Œå‡è®¾ï¼Œå³æœªè§‚æµ‹äº‹å®éƒ½ä¸ºå‡ã€‚è¯·é€æ­¥åˆ†æé—®é¢˜ï¼Œæœ€ç»ˆåªè¾“å‡ºç­”æ¡ˆå¯¹åº”çš„é€‰é¡¹å­—æ¯ï¼Œå¦‚\"A\"ã€‚é¢˜ç›®å¦‚ä¸‹ï¼š\\n\\n### é¢˜ç›®:\\n**é€»è¾‘æ¨ç†æµ‹è¯•é¢˜ï¼š**\\n\\nä»¥ä¸‹æ˜¯å…³äºå‡ ç§ä¸åŒçš„èŒä¸šäººå‘˜çš„å‡è®¾åŠå…¶ç‰¹æ€§çš„æ¨ç†ç³»ç»Ÿã€‚è¯·æ ¹æ®å·²çŸ¥æ¡ä»¶å’Œæ¨ç†è§„åˆ™å›ç­”é—®é¢˜ã€‚\\n\\nå·²çŸ¥æ¡ä»¶ï¼š\\n1. ç©¿ç™½å¤§è¤‚çš„æ˜¯åŒ»ç”Ÿã€‚\\n2. æŒæœ‰æ•™é­çš„æ˜¯æ•™å¸ˆã€‚\\n3. ç©¿åˆ¶æœå¹¶ä¸”æˆ´å¸½å­çš„æ˜¯è­¦å¯Ÿã€‚\\n4. å¦‚æœä¸€ä¸ªäººåœ¨å›¾ä¹¦é¦†å·¥ä½œï¼Œé‚£ä¹ˆä»–æ˜¯å›¾ä¹¦ç®¡ç†å‘˜ã€‚\\n5. å‡è®¾â€œJohnâ€ç©¿ç€ç™½å¤§è¤‚ã€‚\\n6. å‡è®¾â€œEmmaâ€æŒæœ‰æ•™é­ã€‚\\n7. å‡è®¾â€œOliverâ€ç©¿ç€åˆ¶æœå¹¶ä¸”æˆ´ç€å¸½å­ã€‚\\n8. å‡è®¾â€œAliceâ€åœ¨å›¾ä¹¦é¦†å·¥ä½œã€‚\\n\\nè¯·å›ç­”ä»¥ä¸‹é€‰æ‹©é¢˜ï¼š\\n\\n### é—®é¢˜:\\né€‰æ‹©é¢˜ 2ï¼š\\nEmmaçš„èŒä¸šæ˜¯ä»€ä¹ˆï¼Ÿ\\nA. åŒ»ç”Ÿ\\nB. æ•™å¸ˆ\\nC. è­¦å¯Ÿ\\nD. å›¾ä¹¦ç®¡ç†å‘˜\\n<|im_end|>\\n<|im_start|>assistant\\nB<|endoftext|>',\n",
       " 'D<|endoftext|>')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 1800    # Llamaåˆ†è¯å™¨ä¼šå°†ä¸€ä¸ªä¸­æ–‡å­—åˆ‡åˆ†ä¸ºå¤šä¸ªtokenï¼Œå› æ­¤éœ€è¦æ”¾å¼€ä¸€äº›æœ€å¤§é•¿åº¦ï¼Œä¿è¯æ•°æ®çš„å®Œæ•´æ€§\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(f\"<|im_start|>system\\nä½ æ˜¯ä¸€ä¸ªé€»è¾‘æ¨ç†ä¸“å®¶ï¼Œæ“…é•¿è§£å†³é€»è¾‘æ¨ç†é—®é¢˜ã€‚<|im_end|>\\n<|im_start|>user\\n{example['instruction'] + example['input']}<|im_end|>\\n<|im_start|>assistant\\n\", add_special_tokens=False)  # add_special_tokens ä¸åœ¨å¼€å¤´åŠ  special_tokens\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # å› ä¸ºeos tokenå’±ä»¬ä¹Ÿæ˜¯è¦å…³æ³¨çš„æ‰€ä»¥ è¡¥å……ä¸º1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n",
    "    if len(input_ids) > MAX_LENGTH:  # åšä¸€ä¸ªæˆªæ–­\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "# å¤„ç†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "train_ds = train_ds.map(process_func, remove_columns=train_ds.column_names)\n",
    "val_ds = val_ds.map(process_func, remove_columns=val_ds.column_names)\n",
    "num_train_samples = len(train_ds)\n",
    "train_ds, tokenizer.decode(train_ds[0]['input_ids']), tokenizer.decode(list(filter(lambda x: x != -100, train_ds[1][\"labels\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n"
     ]
    }
   ],
   "source": [
    "# è®¾ç½® lora å‚æ•°\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # è®­ç»ƒæ¨¡å¼\n",
    "    r=8, # Lora ç§©\n",
    "    lora_alpha=16,  # Lora alaphï¼Œå…·ä½“ä½œç”¨å‚è§ Lora åŸç†\n",
    "    lora_dropout=0.3  # Dropout æ¯”ä¾‹\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# æŸ¥çœ‹å¯è®­ç»ƒå‚æ•°\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# å¤š GPU åˆ†å¸ƒå¼è®­ç»ƒ\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "#     print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training steps: 1124\n"
     ]
    }
   ],
   "source": [
    "# è‡ªå®šä¹‰ Trainer ç±»\n",
    "class CustomTrainer(Trainer):\n",
    "    def training_step(self, model, inputs):\n",
    "        loss = super().training_step(model, inputs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # æ¢¯åº¦è£å‰ª\n",
    "        return loss\n",
    "\n",
    "# è®¾ç½®è®­ç»ƒè½®æ¬¡å’Œæ‰¹é‡å¤§å°\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "num_train_epochs = 4\n",
    "gradient_accumulation_steps = 4  # 2 æ¬¡å‰å‘ä¼ æ’­æ‰è¿›è¡Œä¸€æ¬¡åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°ï¼Œæœ‰åˆ©äºé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
    "\n",
    "# æŸ¥çœ‹è®­ç»ƒæ€»æ­¥æ•°\n",
    "total_training_steps = (num_train_samples // per_device_train_batch_size // gradient_accumulation_steps) * num_train_epochs\n",
    "print(f\"Total training steps: {total_training_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# è®¾ç½®è®­ç»ƒè¶…å‚æ•°\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"./checkpoints/Qwen2_math_7B_instruct_lora_{TASK_ID}\",\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    evaluation_strategy=\"steps\",  # è®¾ç½®è¯„ä¼°ç­–ç•¥\n",
    "    eval_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    # fp16=True,  # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
    "    load_best_model_at_end=True,  # åœ¨è®­ç»ƒç»“æŸæ—¶åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    metric_for_best_model=\"eval_loss\",  # ç›‘æ§çš„æŒ‡æ ‡\n",
    "    greater_is_better=False,  # å› ä¸ºæˆ‘ä»¬å¸Œæœ›æŸå¤±æœ€å°åŒ–\n",
    "    logging_dir='./logs',  # TensorBoard æ—¥å¿—ç›®å½•\n",
    "    report_to=\"tensorboard\",  # å¯ç”¨ TensorBoard\n",
    "    # dataloader_num_workers=4,\n",
    "    # distributed_data_parallel=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-09 11:52:02,955] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='1124' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 900/1124 1:00:29 < 15:05, 0.25 it/s, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.387700</td>\n",
       "      <td>0.387831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.323100</td>\n",
       "      <td>0.304432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.176500</td>\n",
       "      <td>0.328981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.295358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>0.302061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>0.363511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.432238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.461664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.468061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /data/disk4/home/chenrui/.cache/modelscope/hub/qwen/Qwen2-Math-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /data/disk4/home/chenrui/.cache/modelscope/hub/qwen/Qwen2-Math-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /data/disk4/home/chenrui/.cache/modelscope/hub/qwen/Qwen2-Math-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /data/disk4/home/chenrui/.cache/modelscope/hub/qwen/Qwen2-Math-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=900, training_loss=0.5953734336958991, metrics={'train_runtime': 3635.0512, 'train_samples_per_second': 4.952, 'train_steps_per_second': 0.309, 'total_flos': 1.8455826108716237e+17, 'train_loss': 0.5953734336958991, 'epoch': 3.2})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è®­ç»ƒ å¯ç”¨ CustomTrainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,  # æ·»åŠ éªŒè¯é›†\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],  # è®¾ç½®æ—©åœæ³•ï¼Œè€å¿ƒä¸º 3\n",
    ")\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /data/disk4/home/chenrui/.cache/modelscope/hub/qwen/Qwen2-Math-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "TensorBoard 2.17.0 at http://172.20.2.1:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(f\"./output/Qwen2_math_7B_instruct_lora_{TASK_ID}_final\")\n",
    "!tensorboard --logdir=./logs --host=172.20.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ç”¨å¾®è°ƒåçš„æ¨¡å‹å¯¹éªŒè¯é›†è¿›è¡Œæ¨ç†\n",
    "æ­¤æ—¶å¯ä»¥é‡å¯ Jupyter Kernelï¼Œé‡Šæ”¾æ˜¾å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pdb\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from collections import Counter\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # åªä½¿ç”¨ç¬¬ä¸€å¼ æ˜¾å¡\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")  # æŒ‡å®šä½¿ç”¨çš„è®¾å¤‡\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½ lora æƒé‡\n",
    "model_path = '/data/disk4/home/chenrui/ai-project/Qwen2-7B-Instruct'\n",
    "lora_path = '/data/disk4/home/chenrui/ai-project/logical_reasoning/output/Qwen2_7B_instruct_lora_1_final'  # è¿™é‡Œæ”¹ç§°ä½ çš„ lora è¾“å‡ºå¯¹åº” checkpoint åœ°å€\n",
    "\n",
    "# åŠ è½½æ¨¡å‹\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map={\"\":\"cuda:2\"}, torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path).to(device)  # å°† lora æƒé‡åŠ è¿›åŸæ¨¡å‹\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "# åŠ è½½tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# è¯»å–JSONLæ–‡ä»¶\n",
    "data = []\n",
    "input_file = '/data/disk4/home/chenrui/ai-project/logical_reasoning/data/input/tmp/validation_data.json'\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)  # è¯»å– JSON æ•°æ®\n",
    "print(f\"The size of validation data: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰¹é‡æ¨ç†å¹¶è®¡ç®—æ­£ç¡®ç‡\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for idx, item in enumerate(data):\n",
    "    instruction = item['instruction']\n",
    "    input_text = item['input']\n",
    "    expected_output = item['output']\n",
    "    # match = re.findall(r'[A-G]', output)\n",
    "    # if match:\n",
    "    #     expected_output = match[-1]\n",
    "\n",
    "    prompt = instruction + input_text\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_tensors=\"pt\", return_dict=True).to(device)\n",
    "\n",
    "    gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "    \n",
    "    # å­˜å‚¨å¤šæ¬¡è°ƒç”¨çš„è¾“å‡º\n",
    "    outputs_list = []\n",
    "    \n",
    "    # ä¸‰æ¬¡è°ƒç”¨æ¨¡å‹\n",
    "    for i in range(3):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, **gen_kwargs)\n",
    "            outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "            output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            match = re.findall(r'[A-G]', output)\n",
    "            if match:\n",
    "                output = match[-1]\n",
    "            outputs_list.append(output)\n",
    "            print(f\"Model output {i}: {output}\")\n",
    "\n",
    "    # è¿›è¡Œå¤šè·¯æŠ•ç¥¨\n",
    "    vote_counts = Counter(outputs_list)\n",
    "    final_output = vote_counts.most_common(1)[0][0]  # é€‰æ‹©å‡ºç°æ¬¡æ•°æœ€å¤šçš„ç»“æœ\n",
    "    print(f\"Final voted output: {final_output}, Expected output: {expected_output}\")\n",
    "\n",
    "    # æ¯”è¾ƒé¢„æµ‹ç­”æ¡ˆå’Œæ­£ç¡®ç­”æ¡ˆ\n",
    "    if final_output == expected_output:\n",
    "        correct_count += 1\n",
    "    total_count += 1\n",
    "\n",
    "    # æ¯å¤„ç† n æ¡æ•°æ®è¾“å‡ºä¸€æ¬¡å®æ—¶çš„æ­£ç¡®ç‡\n",
    "    if (total_count) % 50 == 0:\n",
    "        accuracy = correct_count / total_count\n",
    "        print(f\"Processed {total_count} items, Current Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# æœ€ç»ˆæ­£ç¡®ç‡\n",
    "accuracy = correct_count / total_count\n",
    "print(f\"Final Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸…é™¤ç”Ÿæˆçš„ä¸´æ—¶æ–‡ä»¶\n",
    "# if os.path.exists(tmp_dir):\n",
    "#     shutil.rmtree(tmp_dir)\n",
    "#     os.makedirs(tmp_dir)\n",
    "#     print(f'The folder {tmp_dir} has been cleared.')\n",
    "# else:\n",
    "#     print(f'The folder {tmp_dir} does not exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # æ¨¡å‹åˆå¹¶å­˜å‚¨\n",
    "# new_model_directory = \"./merged_model_an\"\n",
    "# merged_model = model.merge_and_unload()\n",
    "\n",
    "# # å°†æƒé‡ä¿å­˜ä¸ºsafetensorsæ ¼å¼çš„æƒé‡, ä¸”æ¯ä¸ªæƒé‡æ–‡ä»¶æœ€å¤§ä¸è¶…è¿‡2GB(2048MB)\n",
    "# merged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datawhale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
