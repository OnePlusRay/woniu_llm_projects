# å¯¼å…¥æ‰€éœ€çš„åº“
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import streamlit as st
from peft import PeftModel
import json
import pandas as pd

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ Yuan2.0 AIç®€å†åŠ©æ‰‹")

# æºå¤§æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='./')

# å®šä¹‰æ¨¡å‹è·¯å¾„
path = './IEITYuan/Yuan2-2B-Mars-hf'
lora_path = './output/Yuan2.0-2B_lora_bf16/checkpoint-51'

# å®šä¹‰æ¨¡å‹æ•°æ®ç±»å‹
torch_dtype = torch.bfloat16 # A10
# torch_dtype = torch.float16 # P100

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    print("Creat tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
    tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)

    print("Creat model...")
    model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch_dtype, trust_remote_code=True).cuda()
    model = PeftModel.from_pretrained(model, model_id=lora_path)

    return tokenizer, model

# åŠ è½½modelå’Œtokenizer
tokenizer, model = get_model()


template = '''
# ä»»åŠ¡æè¿°
å‡è®¾ä½ æ˜¯ä¸€ä¸ªAIç®€å†åŠ©æ‰‹ï¼Œèƒ½ä»ç®€å†ä¸­è¯†åˆ«å‡ºæ‰€æœ‰çš„å‘½åå®ä½“ï¼Œå¹¶ä»¥jsonæ ¼å¼è¿”å›ç»“æœã€‚

# ä»»åŠ¡è¦æ±‚
å®ä½“çš„ç±»åˆ«åŒ…æ‹¬ï¼šå§“åã€å›½ç±ã€ç§æ—ã€èŒä½ã€æ•™è‚²èƒŒæ™¯ã€ä¸“ä¸šã€ç»„ç»‡åã€åœ°åã€‚
è¿”å›çš„jsonæ ¼å¼æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­æ¯ä¸ªé”®æ˜¯å®ä½“çš„ç±»åˆ«ï¼Œå€¼æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«å®ä½“çš„æ–‡æœ¬ã€‚

# æ ·ä¾‹
è¾“å…¥ï¼š
å¼ ä¸‰ï¼Œç”·ï¼Œä¸­å›½ç±ï¼Œå·¥ç¨‹å¸ˆ
è¾“å‡ºï¼š
{"å§“å": ["å¼ ä¸‰"], "å›½ç±": ["ä¸­å›½"], "èŒä½": ["å·¥ç¨‹å¸ˆ"]}

# å½“å‰ç®€å†
query

# ä»»åŠ¡é‡è¿°
è¯·å‚è€ƒæ ·ä¾‹ï¼ŒæŒ‰ç…§ä»»åŠ¡è¦æ±‚ï¼Œè¯†åˆ«å‡ºå½“å‰ç®€å†ä¸­æ‰€æœ‰çš„å‘½åå®ä½“ï¼Œå¹¶ä»¥jsonæ ¼å¼è¿”å›ç»“æœã€‚
'''


# åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
st.chat_message("assistant").write(f"è¯·è¾“å…¥ç®€å†æ–‡æœ¬ï¼š")


# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if query := st.chat_input():

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(query)

    # è°ƒç”¨æ¨¡å‹
    prompt = template.replace('query', query).strip()
    prompt += "<sep>"
    inputs = tokenizer(prompt, return_tensors="pt")["input_ids"].cuda()
    outputs = model.generate(inputs, do_sample=False, max_length=1024) # è®¾ç½®è§£ç æ–¹å¼å’Œæœ€å¤§ç”Ÿæˆé•¿åº¦
    output = tokenizer.decode(outputs[0])
    response = output.split("<sep>")[-1].replace("<eod>", '').strip()

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(f"æ­£åœ¨æå–ç®€å†ä¿¡æ¯ï¼Œè¯·ç¨å€™...")

    st.chat_message("assistant").table(pd.DataFrame(json.loads(response)))
